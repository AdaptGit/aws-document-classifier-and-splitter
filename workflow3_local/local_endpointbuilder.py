
import os
import csv
import boto3
import botocore
import filetype
from json import dumps
from time import sleep
from itertools import repeat
from datetime import datetime
from shutil import rmtree, copytree
from pdf2image import convert_from_path
from concurrent.futures import ThreadPoolExecutor
from textractcaller.t_call import call_textract
from textractprettyprinter.t_pretty_print import get_lines_string


def create_training_dataset(dataset_path, csv_name, bucket_name, _id):
    root_path = os.path.dirname(os.path.abspath(__file__))
    temp_dir_path = root_path + "/workflow1.5_local_temp"
    csv_file_path = temp_dir_path + "/" + csv_name

    # create local directories that store the files generated by this script
    create_temp_directories(temp_dir_path, dataset_path)

    # create training dataset CSV
    with open(csv_file_path, "w", newline="") as file:
        new_image_info = get_processed_images(temp_dir_path)
        items = enumerate(new_image_info)
        with ThreadPoolExecutor() as executor:
            rows = executor.map(add_image_to_csv, items, repeat(temp_dir_path))

        # write rows to CSV
        writer = csv.writer(file)
        writer.writerows(rows)

    # upload CSV to S3 bucket
    s3 = boto3.client("s3")
    s3.upload_file(csv_file_path, bucket_name, csv_name)
    print("Training dataset CSV file has been uploaded to S3")

    # delete the local directories containing the CSV, images, and PDFs
    rmtree(temp_dir_path)
    print("Deleted local CSV, PDFs, and images")


def add_image_to_csv(item, temp_dir_path):
    i = item[0]
    image_info = item[1]
    _class = image_info[1]

    if len(image_info) == 2:
        # extract text from an image that was originally an image
        rel_path = image_info[0]
        image_path = f"{temp_dir_path}/local_dataset_docs/{rel_path}"
        textract_json = call_textract_on_image(image_path)
        raw_text = get_lines_string(textract_json=textract_json)

    elif len(image_info) == 3:
        # extract text from PDF that has been converted into one or more images
        file_name = image_info[0]
        pdf_num_pages = image_info[2]
        # raw_text will store text extracted from all of the images that compose the PDF
        raw_text = ""
        image_path_start = f"{temp_dir_path}/images_processed/{file_name}"

        for page_num in range(pdf_num_pages):
            path = f"{image_path_start}-img-{page_num}.jpg"
            textract_json = call_textract_on_image(path)
            # parse JSON response to get raw text
            image_text = get_lines_string(textract_json=textract_json)
            raw_text += image_text

    # row is returned; contains document's class and the text within it
    print(f"Created row {i + 1}")
    return [_class, raw_text]


def call_textract_on_image(image_path):
    # return JSON response containing the text extracted from the image
    print(f"Inputting {image_path} into Textract")
    textract = boto3.client("textract")
    # send in local image file content (is base64-encoded by API)
    with open(image_path, "rb") as f:
        return call_textract(input_document=f.read(), boto3_textract_client=textract)


def process_file(file_info, temp_dir_path):
    file_path = file_info[0]
    file_extension = file_info[1]

    rel_path = os.path.relpath(file_path, temp_dir_path + "/local_dataset_docs").lstrip("./")
    _class = rel_path.split("/", 1)[0]

    if file_extension == "pdf":
        # create images from PDFs
        # new images, located in the images_processed directory, will be used
        file_name = file_path.split("/")[-1]
        pdf_num_pages = pdf_to_images(file_path, temp_dir_path, file_name)
        image_info = (file_name, _class, pdf_num_pages)
    else:
        # the image stored in the local_dataset_docs directory will be used
        image_info = (rel_path, _class)

    return image_info


def get_processed_images(temp_dir_path):
    valid_file_extensions = {"pdf", "jpg", "png"}
    # file_info is a list of tuples with 2 elements: (file_path, file_extension)
    file_info = []

    # loop through all files in local_dataset_docs
    for root, dirs, files in os.walk(temp_dir_path + "/local_dataset_docs"):
        for file in files:
            file_path = os.path.join(root, file)
            file_type = filetype.guess(file_path)
            if file_type is None:
                # filter out folders
                continue
            file_extension = file_type.extension
            if file_extension in valid_file_extensions:
                # only accept PDF, JPEG, and PNG files
                file_info.append((file_path, file_extension))

    with ThreadPoolExecutor() as executor:
        # new_image_info is a generator of tuples
        # if the file is an image, the tuple is (rel_path, _class)
        # if the file is a PDF, the tuple is (file_name, _class, pdf_num_pages)
        new_image_info = executor.map(process_file, file_info, repeat(temp_dir_path))
    return list(new_image_info)


def create_image(image, image_path_start, i):
    new_image_path = f"{image_path_start}-img-{i}.jpg"
    image.save(new_image_path, "JPEG")


def pdf_to_images(pdf_path, temp_dir_path, file_name):
    # convert PDF into images
    # return number of pages in PDF as an integer
    images = convert_from_path(pdf_path)
    pdf_num_pages = len(images)
    image_path_start = f"{temp_dir_path}/images_processed/{file_name}"
    with ThreadPoolExecutor() as executor:
        executor.map(create_image, images, repeat(image_path_start), range(pdf_num_pages))
    return pdf_num_pages


def create_temp_directories(temp_path, dataset_path):
    try:
        os.mkdir(temp_path)
        os.mkdir(temp_path + "/images_processed")
        copytree(dataset_path, temp_path + "/local_dataset_docs")
    except FileExistsError:
        pass
    print("Created temporary local directories")


def create_comprehend_role(bucket_name, role_name, iam_comprehend_policy_name):
    iam = boto3.client("iam")
    try:
        # create IAM role with trust policy
        iam_assume_role_policy = dumps({
            "Version": "2012-10-17",
            "Statement": {
                "Effect": "Allow",
                "Principal":
                    {"Service": "comprehend.amazonaws.com"},
                "Action": "sts:AssumeRole"
            }
        })
        iam_create_response = iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=iam_assume_role_policy,
            MaxSessionDuration=21600
        )

        role_arn = iam_create_response['Role']['Arn']
        print("IAM role created")

    except botocore.exceptions.ClientError as error:
        # if role already exists
        if error.response["Error"]["Code"] == "EntityAlreadyExists":
            iam_get_role_response = iam.get_role(
                RoleName=role_name
            )
            role_arn = iam_get_role_response["Role"]["Arn"]
            print("IAM role already exists")
        else:
            raise error

    try:
        # create policy that allows role to access the CSV training dataset in S3
        iam_comprehend_policy_document = dumps({
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Action": [
                        "s3:GetObject"
                    ],
                    "Resource": [
                        f"arn:aws:s3:::{bucket_name}/*"
                    ],
                    "Effect": "Allow"
                },
                {
                    "Action": [
                        "s3:ListBucket"
                    ],
                    "Resource": [
                        f"arn:aws:s3:::{bucket_name}"
                    ],
                    "Effect": "Allow"
                },
                {
                    "Action": [
                        "s3:PutObject"
                    ],
                    "Resource": [
                        f"arn:aws:s3:::{bucket_name}/*"
                    ],
                    "Effect": "Allow"
                }
            ]
        })
        iam_create_policy_response = iam.create_policy(
            PolicyName=iam_comprehend_policy_name,
            PolicyDocument=iam_comprehend_policy_document,
        )

        # attach S3 access policy to role
        policy_arn = iam_create_policy_response["Policy"]["Arn"]
        iam.attach_role_policy(
            RoleName=role_name,
            PolicyArn=policy_arn
        )
        print("IAM policy created and attached to role. Waiting to configure")

        # wait for a minute before configuring the Comprehend model
        # IAM role configuration needs time to be processed; without it, the model throws an error
        sleep(60)

    except botocore.exceptions.ClientError as error:
        # if role already exists
        if error.response["Error"]["Code"] == "EntityAlreadyExists":
            print("IAM policy already exists")
        else:
            raise error

    return role_arn


def get_endpoint_arn(csv_file_name, bucket_name, role_arn, model_name):
    comprehend = boto3.client("comprehend")
    region = boto3.session.Session().region_name
    account_number = role_arn.lstrip("arn:aws:iam::").split(":")[0]

    try:
        # create model with the CSV training dataset's S3 URI and the ARN of the IAM role
        create_response = comprehend.create_document_classifier(
            InputDataConfig={
                "S3Uri": f"s3://{bucket_name}/{csv_file_name}"
            },
            DataAccessRoleArn=role_arn,
            DocumentClassifierName=model_name,
            LanguageCode="en"
        )

        model_arn = create_response['DocumentClassifierArn']
        print("Created Comprehend model")

    except botocore.exceptions.ClientError as error:
        # if model already exists
        if error.response["Error"]["Code"] == "ResourceInUseException":
            model_arn = f"arn:aws:comprehend:{region}:{account_number}:document-classifier/{model_name}"
            print("Model has already been created")
        else:
            raise error

    describe_response = comprehend.describe_document_classifier(
        DocumentClassifierArn=model_arn)
    status = describe_response['DocumentClassifierProperties']['Status']

    print("Model training...")
    while status != "TRAINED":
        if status == "IN_ERROR":
            message = describe_response["DocumentClassifierProperties"]["Message"]
            raise ValueError(f"The classifier is in error:", message)
        # update the model's status every 5 minutes if it has not finished training
        sleep(300)
        describe_response = comprehend.describe_document_classifier(
            DocumentClassifierArn=model_arn)
        status = describe_response["DocumentClassifierProperties"]["Status"]

    print("Model trained. Creating endpoint")
    try:
        endpoint_response = comprehend.create_endpoint(
            EndpointName=model_name,
            ModelArn=model_arn,
            DesiredInferenceUnits=10,
        )
        endpoint_arn = endpoint_response["EndpointArn"]

        describe_response = comprehend.describe_endpoint(
            EndpointArn=endpoint_arn
        )
        status = describe_response["EndpointProperties"]["Status"]
        while status == "CREATING":
            if status == "IN_ERROR":
                message = describe_response["EndpointProperties"]["Message"]
                raise ValueError(f"The endpoint is in error:", message)
            # update the endpoint's status every 3 minutes if it has not been created
            sleep(180)
            describe_response = comprehend.describe_endpoint(
                EndpointArn=endpoint_arn
            )
            status = describe_response["EndpointProperties"]["Status"]

    except botocore.exceptions.ClientError as error:
        # if model already exists
        if error.response["Error"]["Code"] == "ResourceInUseException":
            endpoint_arn = f"arn:aws:comprehend:{region}:{account_number}:document-classifier-endpoint/{model_name}"
            print("Endpoint has already been created")
        else:
            raise error
    # the model's endpoint ARN is returned as a string
    return endpoint_arn


def main(bucket_name, dataset_path):
    _id = datetime.now().strftime("%Y%m%d%H%M%S")
    csv_file_name = f"comprehend_dataset_{_id}.csv"
    model_name = f"DatasetBuilderModel-{_id}"
    role_name = "AmazonComprehendServiceRole-" + model_name
    policy_name = model_name + "AccessS3Policy"

    create_training_dataset(dataset_path, csv_file_name, bucket_name, _id)
    role_arn = create_comprehend_role(bucket_name, role_name, policy_name)
    comprehend_endpoint_arn = get_endpoint_arn(csv_file_name, bucket_name, role_arn, model_name)
    return comprehend_endpoint_arn


print("Welcome to the local endpoint builder!\n")
existing_bucket_name = input("Please enter the name of an existing S3 bucket you are able to access: ")
local_dataset_path = input("Please enter the absolute local file path of the dataset folder: ")
print("\nComprehend model endpoint ARN: " + main(existing_bucket_name, local_dataset_path))
